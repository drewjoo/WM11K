{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04000175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_22472\\2823742555.py:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=2)\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#CUDA 메모리 할당 / 디바이스 할당\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6bbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet : residual block이 겹겹이 쌓여 구성된 모델\n",
    "# residual block 정의\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "  def __init__(self,data,label,transform):\n",
    "    super(BasicDataset, self).__init__()\n",
    "    self.data=data\n",
    "    self.label=label\n",
    "    self.transform=transform\n",
    "  def __len__(self):\n",
    "    return len(self.label)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample_X = self.data[idx]\n",
    "    sample_y = self.label[idx]\n",
    "    if self.transform:\n",
    "      sample_X = self.transform(sample_X)\n",
    "    return sample_X, sample_y\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
    "        )\n",
    "\n",
    "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # projection mapping using 1x1conv\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57fb699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=10, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e99cc",
   "metadata": {},
   "source": [
    "# CIFAR10 Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65478c90",
   "metadata": {},
   "source": [
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding = 4)\n",
    "    ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data/vision-greg/cifarpy', train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset=torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "testset=torchvision.datasets.CIFAR10(root='./data/vision-greg/cifarpy', train=False, download=True, transform=transform)\n",
    "\n",
    "def resize_images(dataset, target_size=(64, 64)):\n",
    "    \n",
    "    num_samples, num_channels, _, _ = dataset.shape\n",
    "    resized_dataset = np.zeros((num_samples, num_channels, target_size[0], target_size[1]), dtype=np.uint8)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        for channel in range(num_channels):\n",
    "            # Create a PIL image from the channel data\n",
    "            pil_image = Image.fromarray(dataset[i, channel])\n",
    "\n",
    "            # Resize the image to the target size using PIL\n",
    "            pil_image = pil_image.resize(target_size, Image.ANTIALIAS)\n",
    "\n",
    "            # Convert the resized PIL image back to a NumPy array\n",
    "            resized_dataset[i, channel] = np.array(pil_image)\n",
    "\n",
    "    return resized_dataset\n",
    "    \n",
    "train_data = np.transpose(trainset.data, (0,3,1,2))\n",
    "test_data = np.transpose(testset.data, (0,3,1,2))\n",
    "\n",
    "train_data=resize_images(train_data, target_size = (64,64))\n",
    "test_data=resize_images(test_data, target_size = (64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c424871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.Normalize(mean, std),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding = 4)\n",
    "    ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "# Load cifar10 data\n",
    "train_X = np.load('cifar10_train_data.npy')\n",
    "train_Y = np.load('cifar10_train_label.npy')\n",
    "\n",
    "test_X = np.load('cifar10_test_data.npy')\n",
    "test_Y = np.load('cifar10_test_label.npy')\n",
    "\n",
    "# Into torch.Tensor\n",
    "train_X = torch.Tensor(train_X)\n",
    "train_Y = torch.Tensor(train_Y)\n",
    "\n",
    "test_X = torch.Tensor(test_X)\n",
    "test_Y = torch.Tensor(test_Y)\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset_train = BasicDataset(train_X, train_Y, transform=transform_train)\n",
    "dataset_test = BasicDataset(test_X, test_Y, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af91a660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 3, 64, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7f7b2",
   "metadata": {},
   "source": [
    "# OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669efd1",
   "metadata": {},
   "source": [
    "# data불러오기\n",
    "#OOD = np.load('300K_random_images.npy')\n",
    "\n",
    "# 순서변경\n",
    "#OOD = np.transpose(OOD, (0,3,1,2))\n",
    "\n",
    "# 32*32에서 64*64로 변경하기\n",
    "#OOD = resize_images(OOD, target_size = (64,64))\n",
    "\n",
    "OOD = np.load('300K_random_images_64.npy')\n",
    "# torch.Tensor\n",
    "#OOD = torch.Tensor(OOD)\n",
    "\n",
    "# 임의의 Trarget Labels 만들기\n",
    "desired_size = (len(OOD), 1, 10)\n",
    "OOD_label = torch.zeros(desired_size)\n",
    "\n",
    "# OOD와 OOD_label 합체하기\n",
    "OOD_dataset = BasicDataset(OOD, OOD_label, transform = transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f2b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TinyImage = np.load('300K_random_images_64.npy')\n",
    "sample_indices = np.random.choice(300000, 50000, replace = False)\n",
    "OOD = torch.Tensor(TinyImage[sample_indices])\n",
    "\n",
    "desired_size = (50000, 1, 10)\n",
    "OOD_label = torch.zeros(desired_size)\n",
    "\n",
    "OOD_dataset = BasicDataset(OOD, OOD_label, transform = transform_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677548c5",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d7bac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cdacfb3b70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet34().to(DEVICE)\n",
    "\n",
    "args = {\n",
    "\t'BATCH_SIZE':64,\n",
    "        'LEARNING_RATE': 0.001,\n",
    "        'NUM_EPOCH': 20\n",
    "        }\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE) # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['LEARNING_RATE'])\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c723fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가\n",
    "class SaveBestModel:\n",
    "    def __init__(\n",
    "        self, best_train_loss=float('inf')\n",
    "    ):\n",
    "        self.best_train_loss = best_train_loss\n",
    "\n",
    "    def __call__(\n",
    "        self, current_train_loss,\n",
    "        epoch, model, optimizer, criterion\n",
    "    ):\n",
    "        if current_train_loss < self.best_train_loss:\n",
    "            self.best_train_loss = current_train_loss\n",
    "            print(f\"\\nBest train loss: {self.best_train_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, 'outputs/best_model.pth')\n",
    "\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9e8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습용\n",
    "def train_epoch(model, train_loader_in, train_loader_out, loss_fn, optimizer):\n",
    "    train_loss,train_correct=0.0,0\n",
    "    model.train()\n",
    "    \n",
    "    #for images, labels in dataloader:\n",
    "    for (in_set, out_set) in zip(train_loader_in, train_loader_out):\n",
    "        data = torch.cat((in_set[0], out_set[0]), dim=0)\n",
    "        target = in_set[1]\n",
    "        \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE).reshape(args['BATCH_SIZE'], 10) # CIFAR10은 10으로 변경\n",
    "        \n",
    "        # forward\n",
    "        x = model(data)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = torch.argmax(target, dim = 1)\n",
    "        loss = loss_fn(x[:len(in_set[0])], target)\n",
    "        # cross-entropy from softmax distribution to uniform distribution\n",
    "        #loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * target.shape[0]\n",
    "        predictions = torch.argmax(x[:len(in_set[0])], 1)\n",
    "        \n",
    "        train_correct += (predictions == target).sum().item()\n",
    "        \n",
    "\n",
    "    return train_loss, train_correct\n",
    "\n",
    "def test_epoch(model, dataloader, loss_fn):\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    for images, labels in dataloader:\n",
    "        images,labels = images.to(DEVICE),labels.to(DEVICE).reshape(args['BATCH_SIZE'], 10)# CIFAR10은 10으로 변경\n",
    "        output = model(images)\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "        loss=loss_fn(output, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        predictions = torch.argmax(output, 1)\n",
    "        test_correct += (predictions == labels).sum().item()\n",
    "        y_true += labels.tolist()\n",
    "        y_pred += predictions.tolist()\n",
    "    return test_loss, test_correct, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91fc23ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20 AVG Training Loss:1.976 AVG Training Acc: 25.06 % \n",
      "\n",
      "Best train loss: 1.9759973809814453\n",
      "\n",
      "Saving best model for epoch: 1\n",
      "\n",
      "Epoch:2/20 AVG Training Loss:1.820 AVG Training Acc: 31.81 % \n",
      "\n",
      "Best train loss: 1.8203046623229981\n",
      "\n",
      "Saving best model for epoch: 2\n",
      "\n",
      "Epoch:3/20 AVG Training Loss:1.722 AVG Training Acc: 36.45 % \n",
      "\n",
      "Best train loss: 1.7215575477600098\n",
      "\n",
      "Saving best model for epoch: 3\n",
      "\n",
      "Epoch:4/20 AVG Training Loss:1.620 AVG Training Acc: 40.86 % \n",
      "\n",
      "Best train loss: 1.6196388858032227\n",
      "\n",
      "Saving best model for epoch: 4\n",
      "\n",
      "Epoch:5/20 AVG Training Loss:1.546 AVG Training Acc: 43.96 % \n",
      "\n",
      "Best train loss: 1.5460873715209962\n",
      "\n",
      "Saving best model for epoch: 5\n",
      "\n",
      "Epoch:6/20 AVG Training Loss:1.505 AVG Training Acc: 45.87 % \n",
      "\n",
      "Best train loss: 1.5050262826538086\n",
      "\n",
      "Saving best model for epoch: 6\n",
      "\n",
      "Epoch:7/20 AVG Training Loss:1.473 AVG Training Acc: 47.23 % \n",
      "\n",
      "Best train loss: 1.4733251556396485\n",
      "\n",
      "Saving best model for epoch: 7\n",
      "\n",
      "Epoch:8/20 AVG Training Loss:1.422 AVG Training Acc: 48.83 % \n",
      "\n",
      "Best train loss: 1.4220394140625\n",
      "\n",
      "Saving best model for epoch: 8\n",
      "\n",
      "Epoch:9/20 AVG Training Loss:1.387 AVG Training Acc: 50.50 % \n",
      "\n",
      "Best train loss: 1.3865865041351317\n",
      "\n",
      "Saving best model for epoch: 9\n",
      "\n",
      "Epoch:10/20 AVG Training Loss:1.352 AVG Training Acc: 51.85 % \n",
      "\n",
      "Best train loss: 1.351904861755371\n",
      "\n",
      "Saving best model for epoch: 10\n",
      "\n",
      "Epoch:11/20 AVG Training Loss:1.321 AVG Training Acc: 53.27 % \n",
      "\n",
      "Best train loss: 1.3207761490631102\n",
      "\n",
      "Saving best model for epoch: 11\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_EPOCH\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m----> 8\u001b[0m   train_loss, train_correct\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader_out\u001b[38;5;241m.\u001b[39msampler)\n\u001b[0;32m     10\u001b[0m   train_acc \u001b[38;5;241m=\u001b[39m train_correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader_out\u001b[38;5;241m.\u001b[39msampler) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader_in, train_loader_out, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#for images, labels in dataloader:\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (in_set, out_set) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_loader_in, train_loader_out):\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((in_set[\u001b[38;5;241m0\u001b[39m], out_set[\u001b[38;5;241m0\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      9\u001b[0m     target \u001b[38;5;241m=\u001b[39m in_set[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mBasicDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     17\u001b[0m sample_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel[idx]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 19\u001b[0m   sample_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample_X, sample_y\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load = False # True 활용하면 model만 불러온다.\n",
    "if load == False:\n",
    "    train_loader_in = DataLoader(dataset_train, batch_size=args['BATCH_SIZE'], drop_last=True, shuffle = True)\n",
    "    train_loader_out = DataLoader(OOD_dataset, batch_size=args['BATCH_SIZE'], drop_last = True, shuffle = True)\n",
    "    history = {'train_loss': [], 'train_acc':[]}\n",
    "\n",
    "    for epoch in range(args['NUM_EPOCH']):\n",
    "      train_loss, train_correct=train_epoch(model,train_loader_in, train_loader_out, criterion,optimizer)\n",
    "      train_loss = train_loss / len(train_loader_out.sampler)\n",
    "      train_acc = train_correct / len(train_loader_out.sampler) * 100\n",
    "    \n",
    "      #print(train_correct)\n",
    "      #print(len(train_loader_in.sampler))\n",
    "    \n",
    "      print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Training Acc: {:.2f} % \".format(epoch + 1, args['NUM_EPOCH'],train_loss ,train_acc))\n",
    "      history['train_loss'].append(train_loss)\n",
    "      history['train_acc'].append(train_acc)\n",
    "      save_best_model(\n",
    "          train_loss, epoch, model, optimizer, criterion\n",
    "           )\n",
    "    # load best model\n",
    "    checkpoint = torch.load('outputs/best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # test model using test_epoch function\n",
    "    test_loader = DataLoader(dataset_test, batch_size=args['BATCH_SIZE'], drop_last=True)\n",
    "    test_loss, test_correct, y_true, y_pred = test_epoch(model, test_loader, criterion)\n",
    "\n",
    "    # calculate test set accuracy and loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * test_correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.3f} Test Accuracy: {test_acc:.2f}%\")\n",
    "else:\n",
    "    # load best model\n",
    "    checkpoint = torch.load('outputs/best_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # test model using test_epoch function\n",
    "    test_loader = DataLoader(dataset_test, batch_size=args['BATCH_SIZE'], drop_last=True)\n",
    "    test_loss, test_correct, y_true, y_pred = test_epoch(model, test_loader, criterion)\n",
    "\n",
    "    # calculate test set accuracy and loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * test_correct / len(test_loader.dataset)\n",
    "    print(f\"Test Loss: {test_loss:.3f} Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad867094",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45,fontsize = 10)\n",
    "    plt.yticks(tick_marks, classes,fontsize = 10)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "test_loss, test_correct, y_true, y_pred = test_epoch(model, test_loader, criterion)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "classes = [str(i) for i in range(10)]\n",
    "faulty_case  =['Airplane','Car','Bird','Cat','Deer','Dog','Frog','Horse','Ship','Truck']\n",
    "\n",
    "\n",
    "# Plot the confusion matrix with larger font size\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.rcParams.update({'font.size': 10})  # 폰트 크기 조정\n",
    "plot_confusion_matrix(cm, faulty_case, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789cd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = test_X\n",
    "\n",
    "# 모델에 이미지 데이터 전달\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():  # 그라디언트 계산 비활성화\n",
    "    output = model(sample_image.to(DEVICE))  # 모델에 이미지 데이터 전달\n",
    "\n",
    "# 소프트맥스 함수를 적용하여 확률값 계산\n",
    "softmax_output = torch.softmax(output, dim=1)\n",
    "\n",
    "# 각 이미지별로 가장 큰 소프트맥스 값을 가진 값을 추출\n",
    "softmax_in = torch.max(softmax_output, axis = 1).values\n",
    "softmax_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR95 = np.percentile(softmax_in.cpu().numpy(), q = 5)\n",
    "print(TPR95)\n",
    "plt.hist(softmax_in.cpu().numpy())\n",
    "plt.axvline(x = TPR95, color = 'red')\n",
    "plt.text(TPR95, 500, np.round(TPR95,3), fontsize=12, color='black')\n",
    "plt.title('In Distribution max softmax score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d262475",
   "metadata": {},
   "source": [
    "# Out of Distribution Data load\n",
    "\n",
    "## 1. Mixed Donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c133ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_out = np.load('mixed_wm811k/group_donut.npy')\n",
    "\n",
    "group_out = torch.Tensor(group_out)\n",
    "\n",
    "group_out[group_out == 255] = 1\n",
    "\n",
    "sample_image = group_out\n",
    "\n",
    "# 모델에 이미지 데이터 전달\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():  # 그라디언트 계산 비활성화\n",
    "    output = model(sample_image.to(DEVICE))  # 모델에 이미지 데이터 전달\n",
    "\n",
    "# 소프트맥스 함수를 적용하여 확률값 계산\n",
    "softmax_output = torch.softmax(output, dim=1)\n",
    "\n",
    "predicted_classes = torch.argmax(softmax_output, dim=1).tolist()\n",
    "\n",
    "# 각 이미지별로 가장 큰 소프트맥스 값을 가진 값을 추출\n",
    "softmax_out = torch.max(softmax_output, axis = 1).values\n",
    "print(softmax_out.shape)\n",
    "print(Counter(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(softmax_out.cpu().numpy())\n",
    "plt.axvline(x = TPR95, color = 'red')\n",
    "plt.text(TPR95, 500, np.round(TPR95,3), fontsize=12, color='black')\n",
    "plt.title('Mixed Donut max softmax score - FPR: {:.3f}'.format(np.mean(softmax_out.cpu().numpy() > TPR95)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d8e17c",
   "metadata": {},
   "source": [
    "## 2. Mixed Center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdff3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_out = np.load('mixed_wm811k/group_center.npy')\n",
    "\n",
    "group_out = torch.Tensor(group_out)\n",
    "\n",
    "group_out[group_out == 255] = 1\n",
    "\n",
    "sample_image = group_out\n",
    "\n",
    "# 모델에 이미지 데이터 전달\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():  # 그라디언트 계산 비활성화\n",
    "    output = model(sample_image.to(DEVICE))  # 모델에 이미지 데이터 전달\n",
    "\n",
    "# 소프트맥스 함수를 적용하여 확률값 계산\n",
    "softmax_output = torch.softmax(output, dim=1)\n",
    "\n",
    "predicted_classes = torch.argmax(softmax_output, dim=1).tolist()\n",
    "\n",
    "# 각 이미지별로 가장 큰 소프트맥스 값을 가진 값을 추출\n",
    "softmax_out = torch.max(softmax_output, axis = 1).values\n",
    "print(softmax_out.shape)\n",
    "print(Counter(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82222cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(softmax_out.cpu().numpy())\n",
    "plt.axvline(x = TPR95, color = 'red')\n",
    "plt.text(TPR95, 600, np.round(TPR95,3), fontsize=12, color='black')\n",
    "plt.title('Mixed Center max softmax score - FPR: {:.3f}'.format(np.mean(softmax_out.cpu().numpy() > TPR95)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be1e4f",
   "metadata": {},
   "source": [
    "## 3. Unlike Donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7692bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_out = np.load('mixed_wm811k/group_unlike_donut.npy')\n",
    "\n",
    "group_out = torch.Tensor(group_out)\n",
    "\n",
    "group_out[group_out == 255] = 1\n",
    "\n",
    "sample_image = group_out\n",
    "\n",
    "# 모델에 이미지 데이터 전달\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "with torch.no_grad():  # 그라디언트 계산 비활성화\n",
    "    output = model(sample_image.to(DEVICE))  # 모델에 이미지 데이터 전달\n",
    "\n",
    "# 소프트맥스 함수를 적용하여 확률값 계산\n",
    "softmax_output = torch.softmax(output, dim=1)\n",
    "\n",
    "predicted_classes = torch.argmax(softmax_output, dim=1).tolist()\n",
    "\n",
    "# 각 이미지별로 가장 큰 소프트맥스 값을 가진 값을 추출\n",
    "softmax_out = torch.max(softmax_output, axis = 1).values\n",
    "print(softmax_out.shape)\n",
    "print(Counter(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(softmax_out.cpu().numpy())\n",
    "plt.axvline(x = TPR95, color = 'red')\n",
    "plt.text(TPR95, 800, np.round(TPR95,3), fontsize=12, color='black')\n",
    "plt.title('Unlike Donut max softmax score - FPR: {:.3f}'.format(np.mean(softmax_out.cpu().numpy() > TPR95)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cdcd2",
   "metadata": {},
   "source": [
    "# Error 잡기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f574a",
   "metadata": {},
   "outputs": [],
   "source": [
    " for (in_set, out_set) in zip(train_loader_in, train_loader_out):\n",
    "        data = torch.cat((in_set[0], out_set[0]), dim=0)\n",
    "        target = in_set[1]\n",
    "        \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE).reshape(args['BATCH_SIZE'], 8) # CIFAR10은 10으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56548a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " for (in_set, out_set) in zip(train_loader_in, train_loader_out):\n",
    "        data = torch.cat((in_set[0], out_set[0]), dim=0)\n",
    "        target = in_set[1]\n",
    "        \n",
    "        data, target = data.to(DEVICE), target.to(DEVICE).reshape(args['BATCH_SIZE'], 8) # CIFAR10은 10으로 변경\n",
    "        \n",
    "        # forward\n",
    "        x = model(data)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = torch.argmax(target, dim = 1)\n",
    "        loss = criterion(x[:len(in_set[0])], target)\n",
    "        # cross-entropy from softmax distribution to uniform distribution\n",
    "        loss += 0.5 * -(x[len(in_set[0]):].mean(1) - torch.logsumexp(x[len(in_set[0]):], dim=1)).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * target.shape[0]\n",
    "        predictions = torch.argmax(x[:len(in_set[0])], 1)\n",
    "        \n",
    "        train_correct += (predictions == target).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73001c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[len(in_set[0]):].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd7a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.logsumexp(x[len(in_set[0]):], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c87669",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[len(in_set[0]):].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.logsumexp(x[len(in_set[0]):], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc85f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
